#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Convert a CSV file to wiki pages.  Run 'csv2wiki --help' for details.
# https://github.com/OpenTechStrategies/ots-tools/blob/master/csv2wiki
#
# Copyright (C) 2017 Open Tech Strategies, LLC
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

__doc__ = """\
Convert each row of a UTF-8 CSV file to a MediaWiki page.

Basic usage:

  $ csv2wiki -c CONFIG_FILE [OPTIONS] (CSV_FILE | --delete-matching=PATTERN)

The common case is to pass a CSV_FILE on the command line; wiki pages
will then be created (or updated) based on the CSV contents.  (For the
less common page-deletion case, see section "Page deletion support".)

This script expects the cells in CSV_FILE to contain HTML snippets.
The cells need not be valid HTML documents, but they should be
formatted with balanced HTML tags as opposed to, say, Markdown format.

The CONFIG_FILE contains the wiki URL, login information, and various
other run-time parameters.  It is in standard .ini file format, with
a sole "[default]" section and the following elements in that section:

  wiki_url:            The url of the wiki, e.g. "http://localhost/mediawiki",
                       "https://www.example.com/mwiki", etc.

  username:            User account with write/create permission in the wiki.
                       (With --delete, user needs page-delete permission.)

  password:            The wiki password corresponding to the username.

  toc_name:            Title for the generated Table of Contents page.

  title_tmpl:          Template into which selected column values from
                       a row are substituted to create the title of
                       each row's page.  Columns are specified with
                       "{N}" in the string, where N is a column number.
                       The special column {0} is the CSV row number,
                       which is automatically left-padded with zeros
                       appropriately for the total number of rows.

                       For example:

                       Suppose title_tmpl is "Entry_{1}_{3}_{0}",
                       column 1 contains "foo", column 3 has "bar",
                       and this is row 15 of a CSV file with 250 rows.
                       The page title corresponding to this row would
                       be "Entry_foo_bar_015".

                       *NOTE:* 

                       The titles generated this way must be unique.
                       Typically, you ensure uniqueness by making sure
                       that at least one element of title_tmpl is some
                       kind of unique identifying number -- either a
                       column whose value is known to be unique, or
                       the row number.

                       If the generated page titles are not unique,
                       only the last row in the set of duplicates will
                       appear in the wiki.  (TODO: we could solve this
                       by checking for collisions and adding a unique
                       tail when one happens, but we currently don't
                       do that.  So, be careful.)

  cat_col:             The number of the column (if any) in the CSV
                       file that should be used to create a category
                       for that row; column numbering begins at 1, not
                       0.  Omit this, or leave the value blank, to not
                       use categories at all.

  col_map:             A map to reorder/rename/regroup columns into sections.

                       This is a versatile and multifaceted option --
                       the core of csv2wiki, really.  Spend some time
                       to get to know it.  You'll be glad you did.

                       The col_map is a typically multi-line list of
                       column numbers, given in the desired order for
                       the corresponding sections in the wiki page,
                       with optional new section titles to replace the
                       default column heading.

                       Each line of col_map is a specifier of this form:

                         COL_SPEC   OPTIONAL SECTION TITLE FOR THAT COLUMN

                       COL_SPEC contains no whitespace, and any amount
                       of whitespace may separate it from the optional
                       section title string.  If no section title is
                       given, then the original column header of that
                       column is used for the section title.

                       The order in which columns are specified, from
                       top to bottom in the col_map list, is the order
                       in which corresponding sections in each wiki
                       page will be created.

                       COL_SPEC can be one of three things:

                         1) A column number (the simplest case).
  
                         2) A sequence of numbers separated by dots,
                            e.g., "30.27".  This creates a subsection in
                            the wiki page.  The last component ("27" here)
                            is the column being specified, and the order
                            in which the line appears in col_map just
                            shows that subsection's place *within* the
                            overall encapsulating section ("30").
  
                         3) A string containing no dots and at least one
                            non-numeric.  This creates a new section,
                            not corresponding to any column in the CSV,
                            so that you can group other subsections
                            within it.

                       (TODO: generalize above to COL_SPEC.COL_SPEC etc.)

                       Here is a complete example specification:

                         col_map:   3           Applicant Name
                                   12           Executive Summary
                                   15           Detailed Proposal
                                   comments     Comments
                                   comments.23  Reviewer CC Comments
                                   comments.21  Reviewer BB Comments
                                   comments.20  Reviewer AA Comments
                                   30           Total Score
                                   30.29        Reviewer CC Score
                                   30.28        Reviewer BB Score
                                   30.27        Reviewer AA Score

                       These are the wiki sections the above would produce:

                         + Applicant Name
                         + Executive Summary
                         + Detailed Proposal
                         + Comments
                           - Reviewer CC Comments
                           - Reviewer BB Comments
                           - Reviewer AA Comments
                         + Total Score
                           - Reviewer CC Score
                           - Reviewer BB Score
                           - Reviewer AA Score

                       So even if the original CSV had the header
                       "Appl. Name" for column 3 and "Exec. Summ." for
                       column 12, the wiki has "Applicant Name" and
                       "Executive Summary", and so on.

                       You can also use this to omit columns from the
                       conversion entirely: if you don't list a
                       column, it simply won't be included as a wiki
                       section at all.

                       No column number may be repeated; if there are
                       repeats, the behavior is undefined.

                       The column numbers used in the spec always
                       refer to the original column numbers in the
                       CSV; any re-ordering is applied afterwards.

                       Any reordering or omissions done in col_map do
                       not affect column references in other run-time
                       configuration parameters.  In other words, all
                       column numbers specified in the config file
                       refer to the original CSV file, not to the
                       reordering used for the wiki output.

  last_cat:            The (case-insensitive) name of the category, if
                       any, that should always be listed last in the
                       table of contents, even if that category contains
                       more pages than some other category.

  keep_empty:          If present, still create sections for empty
                       cells in the CSV.  If a value is provided, use
                       that value as the content of the section;
                       otherwise, use the empty string.

                       In omitted, then simply don't create a section
                       within a row's page for any column (cell) that
                       has no content; this is the default behavior.

  delimiter:           A single-char delimiter used to separate columns
                       in a CSV row.  If omitted, defaults to ','.

  quotechar:           A single-char quotechar used to wrap contents of
                       a single cell in the CSV file.  If omitted, 
                       defaults to '"'.

  path_to_api:         The API path under the URL; defaults to "/"; see
                       mwclient.readthedocs.io/en/master/user/connecting.html.
                       Note that on Wikipedia.org, this would be "/w/", 
                       but in most non-Wikipedia instances of MediaWiki,
                       the default "/" is more likely.  So if you're using
                       that default, you don't have to specify this at all.

Example config file
-------------------

Here is an example config file:

  [default]
  wiki_url: http://localhost/mediawiki
  username: wikibot
  password: bqaRY76gtXu
  title_tmpl: Entry_{1}
  toc_name: List_of_Entries
  cat_col: 5
  col_map:   3           Applicant Name
            12           Executive Summary
            15           Detailed Proposal
            comments     Comments
            comments.23  Reviewer CC Comments
            comments.21  Reviewer BB Comments
            comments.20  Reviewer AA Comments
            30           Total Score
            30.29        Reviewer CC Score
            30.28        Reviewer BB Score
            30.27        Reviewer AA Score

The "[default]" section name at the top must be present.  The .ini
format always has sections, and for the sake of forward compatibility
this script requires the one section's name to be "default".

Command-line options:
---------------------

In general, the config file is for run-time configuration that is
likely to be permanent (e.g., used for a production run), and
command-line options are for optional behavior that is likely to vary
from run to run.  The exception to this is the "-c" option, of course,
since it's how the config file is indicated in the first place.

  -c | --config FILE   Use FILE as the run-time job control file;
                       see the "Example config file" section above.

  --null-as-value      When a cell's entire content is the word
                       "null" (matched case-insensitively), then treat
                       the cell as having that literal content,
                       instead of treating the cell as being empty.
                       (The latter is the default because in most CSV
                       files, if a cell just contains the word "null",
                       that's just a kind of conversion error and is
                       really an indication that the cell is empty.)

  --pare N             Convert only 1/N of the CSV rows to wiki pages.
                       This is useful for doing test conversions of a
                       large CSV file, when you don't want to convert
                       every row in every run.  The --pare option will
                       convert just 1/N rows, spaced evenly across the
                       CSV file and selected deterministically.

  --show-columns       Just print all the column headers in the CSV
                       (that is, the values in the first row) to
                       stdout, with each column numbered.  

                       This is a convenience option to aid in writing
                       a config file, and if passed, must be the only
                       option, since it just prints information and
                       exits, without creating any wiki pages.

  --delete-matching    See "Page deletion support" section below.

  -h | --help          Show usage message and exit without error.

Page deletion support
---------------------

This script also supports batch deletion of pages, since a possible
outcome of running this is that you end up with pages in your wiki
that you don't want.  Use the "--delete-matching PATTERN" option to
remove them.  All pages whose names match PATTERN will be deleted (and
no pages will be created).

Typically, PATTERN will be similar to the title_tmpl config element,
since --delete-matching is most often used to delete pages originally
created with this script.

   (TODO: Page deletion does not yet de-list the removed pages from
          the generated TOC page.  We should fix that.)

Dependencies and Troubleshooting
--------------------------------

* This requires the 'bs4', 'mwclient', 'unidecode' Python libraries

  If you get an error that looks something like this:

    Traceback (most recent call last):
      File "./csv2wiki", line 332, in <module>
        from bs4 import BeautifulSoup
    ImportError: No module named 'bs4'

  or this:

    Traceback (most recent call last):
      File "./csv2wiki", line 27, in <module>
        from mwclient import Site
    ImportError: No module named mwclient

  or this:

    Traceback (most recent call last):
      File "./csv2wiki", line 27, in <module>
        from unidecode import unidecode
    ImportError: No module named unidecode

  then run these commands

    $ sudo pip3 install bs4
    $ sudo pip3 install mwclient
    $ sudo pip3 install unidecode

  and try again.

* You may need to convert carriage returns to linefeeds

  If your CSV input file uses only carriage returns (CR) for line
  breaks, then you must convert it to using line feeds (LF).  On
  Unix-like systems, this should work:

    $ tr '\\r' '\\n' < cr-file.csv > lf-file.csv

  Note this should not be necessary if the CSV file is in CRLF format,
  that is, the Microsoft Windows / MS-DOS standard for line endings,
  which uses both characters together at the end of each line.

* If you run create/delete multiple times, you may need to run
  
    $ php maintenance/rebuildall.php
  
  in your MediaWiki instance to link pages to their categories
  properly.  That script took about 10 minutes to run for a wiki with
  <300 pages, so be prepared to wait.  Also, as of early 2017 on
  Debian GNU/Linux, one of the authors had to run 'sudo apt-get
  install php7.0-mysql' to enable rebuildall.php to work.

  Sources:
  
  - "Categories aren't working."
    https://www.mediawiki.org/wiki/Topic:T6uzpn51mgb8n5sc

  - "Database connection fails."
    https://www.mediawiki.org/wiki/Thread:Project:Support_desk/\\
    MediaWiki_upgrade_fails_with_Database_error/reply

* If you get errors saving some pages, it may be an anti-spam plugin.
  
  If your MediaWiki instance has Extension:SpamBlacklist enabled,
  then you may get errors when trying to create pages that contain
  certain kinds of URLs or email addresses (namely, URLs or email
  addresses that SpamBlacklist thinks look spammy). 
  
  One solution is to just turn off Extension:SpamBlacklist entirely.
  But even if you don't have that kind of administrative access,
  you might still have enough access to *configure* the extension, 
  in which case you can whitelist everything via a catchall regexp.
  Visit one or of of these pages:
  
    https://mywiki.example.com/index.php?title=MediaWiki:Spam-whitelist
    https://mywiki.example.com/index.php?title=MediaWiki:Email-whitelist
  
  You'll see a commented-out explanation of how the whitelist works.
  Just add a line with the regular expression ".*", as in this example:
  
    # External URLs matching this list will *not* be blocked even if they would
    # have been blocked by blacklist entries.
    #
    # Syntax is as follows:
    #   * Everything from a "#" character to the end of the line is a comment
    #   * Every non-blank line is a regex fragment which will only match hosts inside URLs
    .*
  
  That will let you save a page containing any URL.  (Things work
  similarly on the Email-whitelist page).
"""

import csv
import mwclient
import unidecode
import getopt, sys
import configparser
import re
import warnings
from bs4 import BeautifulSoup

# For exception matching.
import requests

class WikiSectionSkel():
    """One section, subsection, etc of a wiki page.  This is the
    usual DAG tree structure; the top element -- the one with no
    parent and no sec_title -- represents the wiki page as a whole.
    (Overall, the entire tree is effectively the data structure
    representation of the 'col_map' option in the config file.)"""
    def __init__(self, parent=None, column=None, sec_title=None):
        """Create one (sub)section on a wiki page.  

        If PARENT is None, then this instance is the top element of a
        page tree, meaning this instance represents the page itself,
        so COLUMN and SEC_TITLE must also be None.

        Otherwise: 

          - PARENT is the WikiSectionSkel for the encapsulating
            section that this (sub)section is in.

          - COLUMN is either a number (the column in the original csv
            file that corresponds to this section) or a string
            (representing a new section created for grouping
            subsections within it).

          - SEC_TITLE is either None, meaning just use the column
            header taken from the CSV, or a string to use as this
            section's title.  But when COLUMN is a string, then
            SEC_TITLE must also be a string, since in that case there
            is no corresponding column header from the CSV to use as
            the section title.  (While we could use the empty string
            in that case, that feels a bit too relaxed.)"""
        self.parent = parent
        self.column = column
        self.sec_title = sec_title

        if self.parent is not None:
            self.level = self.parent.level + 1
            self.parent.children.append(self)
        elif self.column is not None or self.sec_title is not None:
            raise Exception("ERROR: no parent, but column or sec_title given")
        else:
            self.level = 0
        # Ordered list of sections underneath this one.
        self.children = []

    def __str__(self):
        """String representation, normally used only for debugging."""
        pad = "  " * self.level
        title = "Untitled wiki section"
        parent = "(no parent)"
        column = 0
        if self.sec_title:
            title = self.sec_title
        if self.parent and self.parent.sec_title:
            parent = self.parent.sec_title
        if self.column:
            column = self.column
        return "" \
            + pad + "WikiSectionSkel '%s':\n" % title \
            + pad + "  parent:    %s\n" % parent \
            + pad + "  level:     %d\n" % self.level \
            + pad + "  column:    %s\n" % column \
            + pad + "  children:  %d\n" % len(self.children)


class WikiSession:
    """One session loading a CSV file into a wiki."""
    def __init__(self, config, csv_input):
        """Start a wiki session, taking login parameters from CONFIG and
        column header information (if needed) from CSV_INPUT."""
        self.site_conn           = None  # will be a mwclient Site object
        self.wiki_url            = config['wiki_url']
        self.username            = config['username']
        self.password            = config['password']
        self.title_tmpl          = config['title_tmpl']
        self.toc_name            = config['toc_name']
        self.cat_col             = int(config.get('cat_col', 0))
        self.last_cat            = config.get('last_cat', None)
        self.keep_empty          = config.get('keep_empty', False)
        self.path_to_api         = config.get('path_to_api')
        self.wiki_page_skel      = WikiSectionSkel()
        # This gets inhaled into self.wiki_page_skel later:
        col_map                  = config.get('col_map', None) 

        if self.path_to_api is None:
            self.path_to_api = "/"

        if self.last_cat is not None:
            # It's always used case-insensitively and w/o surrounding spaces
            self.last_cat = self.last_cat.strip().lower()

        self.csv2wiki_url = 'https://github.com/OpenTechStrategies/' \
                            + 'ots-tools/blob/master/csv2wiki'

        if col_map is not None:
            # col_map is a serious parsing job: it needs to be turned
            # into a WikiSectionSkel tree.
            cur_parent = self.wiki_page_skel
            int_matcher = re.compile("^[0-9]+$")

            # Fill out wiki_page_skel to match col_map.
            # 
            # Because of the ConfigParser syntax, the format we get the
            # map in is one big string, splittable on line breaks into
            # a list of lines, where each line starts with a column spec,
            # maybe followed by whitespace, optionally followed by a
            # new title for the corresponding section.
            for line in col_map.splitlines():
                col_spec = None
                new_title = None

                try:
                    col_spec, new_title = line.split(None, 1)
                except ValueError:
                    col_spec = line

                elts = col_spec.split(".")

                i = len(elts) - 1
                if i == cur_parent.level:
                    pass  # normal case: make another section in this parent
                elif i == cur_parent.level + 1:
                    if len(cur_parent.children) == 0:
                        raise Exception("ERROR: trying to make a subsection "
                                        + "of a non-existent section")
                    cur_parent = cur_parent.children[-1]
                elif i > cur_parent.level:
                    raise Exception("ERROR: section nesting jumped "
                                    + "by more than one level")
                else:  # i <= cur_parent.level
                    while(cur_parent.level > i):
                        cur_parent = cur_parent.parent

                col_num = elts[-1]

                if int_matcher.match(col_num):
                    col_num = int(col_num)
                elif new_title is None:
                    raise Exception("ERROR: section title required " \
                                    + "for '%s'" % elts[0])

                new_skel = WikiSectionSkel(cur_parent, col_num, new_title)
        else:  # no col_map provided, so contruct trivial one from headers
            i = 1
            for header in csv_input.headers:
                WikiSectionSkel(self.wiki_page_skel, i, header)
                i += 1
            
        # Connect to the site.
        try:
            self.site_conn = mwclient.Site(self.wiki_url.split("://"), path=self.path_to_api)
        except requests.exceptions.HTTPError as err: 
            sys.stderr.write("ERROR: failed to connect to wiki URL '%s'\n" % self.wiki_url)
            sys.stderr.write("       Error details:\n")
            sys.stderr.write("       ('%s')\n" % err)
            sys.exit(1)
    
        try:
            self.site_conn.login(self.username, self.password)
        except mwclient.errors.LoginError as err:
            sys.stderr.write("ERROR: Unable to log in to wiki; "
                             "check that username and password are correct.\n")
            sys.stderr.write("       Error details:\n")
            sys.stderr.write("       ('%s')\n" % err)
            sys.exit(1)

    def wiki_escape(self, s):
        """Return a wiki-escaped version of STRING."""
        # There was a page with a title like
        # "Entry_72_Foo_Bar_]Baz[." and the API rejected
        # that, so I guess we have to do some escaping.  
        # Later some fields with "#N/A" failed (as text in a page, not
        # as page titles).  Below we carelessly lump all of these
        # cases together and use "-" as the go-to replacement.
        return s.replace("]", "-").replace("[", "-").replace("#", "-").replace("/", "-")

    # Thse are completeley MediaWiki-specific right now.  We could
    # conditionalize based on a new config parameter 'wiki_type'.
    def write_page(self, page_name, text):
        """Make page PAGE_NAME in this wiki have TEXT,
        with the standard colophon appended."""
        # Put a colophon at the end of every page, because users need to
        # know that the page was auto-generated.  For one thing, that
        # might make them think twice about manually editing it, lest
        # their changes be overwritten by a subsequent run of the script.
        colophon = "\n\n"                                           \
                   + '<span style="font-size:75%" >'                \
                   + "'''Colophon:''' This page was generated by "  \
                   + '[' + self.csv2wiki_url + ' csv2wiki]. '       \
                   + 'Manual changes to this page might be '        \
                   + 'overwritten by a subsequent run of csv2wiki.' \
                   + '</span>'                                      \
                   + '\n'
        # The log message (edit message, commit message, whatever): the
        # metadata you record in MediaWiki whenever you submit a change.
        edit_msg = "Page generated by csv2wiki (" + self.csv2wiki_url + ")."
        page = self.site_conn.pages[page_name]
        try:
            page.save(text + colophon, edit_msg)
        except mwclient.errors.APIError as e:
            raise Exception("ERROR: unable to write page: '%s'" % e.info)

    def delete_page(self, page_name):
        """Delete page PAGE_NAME from this wiki."""
        page = self.site_conn.pages[page_name]
        try:
            page.delete()
        except mwclient.errors.APIError as e:
            raise Exception("ERROR: unable to delete page: '%s'" % e.info)

class CSVInput():
    """Iterator class encapsulating a CSV file as input."""
    def count_rows(self):
        """Count rows in the csv file, accounting for the headers"""
        
        self.csv_fh.seek(0)
        row_count = sum(1 for row in self.csv_reader) - 1
        
        # Reset the reader for our callers.
        self.csv_fh.seek(0)
        self.csv_reader = csv.reader(self.csv_fh,
                            delimiter=self.config.get('delimiter', ','),
                            quotechar=self.config.get('quotechar', '"'))
        
        return row_count
    
    def __init__(self, csv_file, config):
        """Prepare CSV_FILE for input, with delimiters from CONFIG.
        CONFIG is a dict returned from parse_config_file(), or else
        it is None, in which case default config values are used."""
        self.csv_reader          = None  # will be csv.reader object
        self.headers             = []
        self.row_count           = None  # will be num rows not counting header
        
        self.config = config or {}
        self.csv_fh = open(csv_file)
        self.csv_reader = csv.reader(self.csv_fh,
                            delimiter=self.config.get('delimiter', ','),
                            quotechar=self.config.get('quotechar', '"'))       

        # First get a row count, so we can pad the row number later.
        self.row_count = self.count_rows()

        # Set column headers
        self.headers = next(self.csv_reader)
        
    def show_columns(self, out):
        """Print this CSV's column names, numbered, to OUT."""
        fmt = "{:0" + str(len(str(len(self.headers)))) + "}"
        for i in range(len(self.headers)):
            out.write("%s) %s\n" % (fmt.format(i + 1), self.headers[i]))

    def __iter__(self):
        """Return the underlying iterator for this CSV's rows."""
        return self.csv_reader

    def __next__(self):
        """Return the next row in this CSV."""
        return next(self.csv_reader)
    
def wikify_anchors(soup):
    """Return new soup with all of SOUP's href-bearing anchors wikified.
    That is, anchors of the form <a href="url">text</a> in SOUP will
    be converted to MediaWiki-style "[url text]".

    SOUP, of course, is beautiful soup, because who has time for ugly
    soup?

    """
    for a in soup.select('a'):
        if 'href' in str(a):
            a.replace_with(
                BeautifulSoup("[{1} {0}]".format(a.text, a['href']), 
                              'html.parser'))
    return soup

def format_cell(cell):
    """Take the html in CELL and adjust it to be mediawiki-friendly.  This
is mediawiki-specific, but I imagine a future version of this might
want to override with formatting for other wikis.

    """

    # Mediawiki doesn't do tbody
    cell = cell.replace("<tbody>", "").replace("</tbody>", "")
    
    # Make soup
    warnings.filterwarnings("ignore",
                            category=UserWarning, module='bs4')
    soup = BeautifulSoup(cell, "html.parser")
    
    soup = wikify_anchors(soup)
    
    return str(soup)


def create_pages(wiki_sess, csv_input, null_as_value, pare):

    """Create a wiki page in WIKI_SESS for each row in CSV_INPUT.
    The CSV must have at least one row of content.

    If NULL_AS_VALUE, then when a CSV cell's entire content is
    (case-insensitively) the word "null", treat the cell as having
    that literal content, instead of treating the cell as empty.

    If PARE is not None, it is an integer indicating that only
    1/PARE rows should be handled, and the rest skipped."""

    # Map category names to lists, where the elements of each list are
    # the titles of the pages in the corresponding category.  The
    # special category "" is used for pages that have no category.
    # (If categories are not in use at all, or if no pages have a
    # category, then all pages would be listed under "".)
    categories = {}

    def massage_string(s):
        """Convert non-ASCII string S to nearest lower ASCII equivalent."""
        # TODO: This is really a todo for the unidecode module
        # (https://pypi.python.org/pypi/Unidecode), not us.  The
        # documentation says to just use the unidecode() function as
        # the entry point.  That's an alias for
        # unidecode_expect_ascii(), which should be the right choice
        # when most of your input is the plain ASCII subset of UTF-8,
        # because it tries to decode using an ASCII assumption and
        # then catches the exception and tries again with non-ascii as
        # the assumption in the rare case that the string is not
        # ASCII.
        #
        # However, I think maybe it has a bug with respect to recent
        # Python versions?  It tries to trap the exception by catching
        # 'UnicodeEncodeError', but the error actually thrown is
        # (reasonably enough) 'UnicodeDecodeError'.  So it just
        # propagates that exception up the stack and never makes it to
        # the unidecode_expect_nonascii() call that would have
        # successfully decoded the string.
        # /usr/local/lib/python2.7/dist-packages/unidecode/__init__.py
        # has the details (on my system, at least).
        #
        # Anyway, the solution is to call unidecode_expect_nonascii()
        # directly and not worry that that's slightly slower overall.
        return unidecode.unidecode_expect_nonascii(s)

    # read in csv
    row_num = 0
    for row in csv_input:
        row_num += 1
        page = None
        page_name = None
        page_text = ""
        categorized_p = False

        if pare is not None and row_num % pare != 0:
            continue

        wiki_sess.emit_page(row, row_num)
        fooo
        # Looping over the cells in the row.  Name the sections
        # according to headers.
        cell_num = 0
        cell_text = None
        for cell in reordered_row:
            if cell_num == 0:
                # For this new line, generate a mediawiki page
                #
                # Splice in any requested columns.  We prepend the
                # row_num as the first element in the tuple, which
                # brings two benefits.  First, the user get access
                # to the row number, via the special code "{0}" in
                # title_tmpl in the config file.  Second, all the
                # other columns, which the user is likely to think
                # of using 1-based indexing not 0-based, are now
                # 1-based, because the row number is in the first
                # slot (slot [0]) of the tuple. 
                row_num_fmt = "{:0" + str(len(str(csv_input.row_count))) + "}"
                row_num_str = row_num_fmt.format(row_num)
                page_name = wiki_sess.title_tmpl.format(*([row_num_str,] + row))
                # The input is UTF-8, but for wiki page names we
                # want to stick to plain old lower ASCII.
                page_name = massage_string(page_name)
                page_name = page_name.replace(" ", "_")
                page_name = wiki_sess.wiki_escape(page_name)

                # Wikimedia will convert &ampl to an ampersand and
                # then consider that ampersand as the start of a new
                # ampersand-encoded special char and then complains
                # about an invalid page title.  We put underscores
                # fore and aft to break up that second special char.
                if re.search(r"&amp;.*;", page_name):
                    page_name = page_name.replace("&amp;", "_&amp;_")
                    page_name = page_name.replace("&amp;__", "&amp;_")
                    page_name = page_name.replace("__&amp;", "_&amp;")
                
                # Set the contents of each cell to their own section.
            # Unconditionally increment the cell number; note this
            # also switches us from 0-based to 1-based indexing.
            cell_num += 1
            # Do certain things if the cell is effectively empty.
            if cell is "" or ((not null_as_value) and cell.lower() == "null"):
                # If the cell is effectively empty, do we skip it or
                # use it?  The only circumstance under which we skip
                # is if 'wiki_sess.keep_empty' is exactly False;
                # otherwise, it's either True or is some string that
                # we should use as the value for an empty cell.
                if wiki_sess.keep_empty is False:
                    continue
                else:
                    if isinstance(wiki_sess.keep_empty, str):
                        cell_text = wiki_sess.keep_empty
                    else:
                        cell_text = ""
            else:
                cell_text = cell
            if cell_num == wiki_sess.cat_col:
                cell_esc = wiki_sess.wiki_escape(massage_string(cell))
                cell_text = '[[:Category:' + cell_esc + '|' + cell_esc + ']]\n'
                cell_text += '[[Category:' + cell_esc + ']]'
                categorized_p = True
                if categories.get(cell_esc) is None:
                    categories[cell_esc] = [page_name]
                else:
                    categories[cell_esc].append(page_name)

            ## Clear up formatting issues
            cell_text = format_cell(cell_text)

            # It's [cell_num - 1] below because we switched to 1-based
            # indexing earlier.
            page_text += "== " + reordered_headers[cell_num - 1] + " =="
            page_text += "\n\n"
            page_text += cell_text
            page_text += "\n\n"
        if not categorized_p:
            # If this row did not fall into any category, then put it
            # in the magical category whose name is the empty string.
            if categories.get("") is None:
                categories[""] = [page_name]
            else:
                categories[""].append(page_name)
        wiki_sess.write_page(page_name, page_text)
        print(("CREATED PAGE: \"" + page_name + "\""))
        
    # create the TOC page.
    toc_text = ""
    num_categories = len(list(categories.keys()))

    # This got too big to fit into a lambda anymore :-).
    def categories_sorter(key):
        "Return a descending sort value for category KEY."
        if (wiki_sess.last_cat is not None 
            and key.strip().lower() == wiki_sess.last_cat):
            return 0
        else:
            return -len(categories[key])
    
    for cat in sorted(list(categories.keys()), key=categories_sorter):
        if num_categories > 1:
            usable_cat = cat
            if usable_cat == "":
                # Yup, we're just going to hardcode this right here.
                # It's a very rare case: the conversion is using
                # categories, and we have more than one category
                # available, but some pages had no category.  So what
                # category should they go in (given that the TOC will
                # be organized into categories because multiple
                # categories are available)?  Answer: make one up.
                usable_cat = "csv2wiki Miscellaneous Default Category"
            usable_cat = usable_cat + " (" + str(len(categories[cat])) + ")"
            toc_text += "==== " + usable_cat + " ====\n\n"
        for pnam in categories[cat]:
            toc_text += '* [[' + pnam + ']]\n'
        toc_text += "\n"
    wiki_sess.write_page(wiki_sess.toc_name, toc_text)
    print(("CREATED TOC: \"" + wiki_sess.toc_name + "\""))
    
    # generate the category pages
    if wiki_sess.cat_col:
        for category in list(categories.keys()):
            wiki_sess.write_page('Category:' + category, "")
            print(("CREATED CATEGORY: \"" + category + "\""))

    return

def delete_pages(wiki_sess, pat):
    """Delete wiki pages in WIKI_SESS's wiki whose names match pattern PAT."""
    # Could change what="title" to what="text" to get full-text
    # matching on page contents.  We might want to offer that option.
    search_results = wiki_sess.site_conn.search(pat, what="title")
    for result in search_results:
        wiki_sess.delete_page(result['title'])
        print(("DELETED PAGE: \"" + result['title'] + "\""))
    return

def usage(errout=False):
    """Print a message explaining how to use this script.
    Print to stdout, unless ERROUT, in which case print to stderr."""
    out = sys.stderr if errout else sys.stdout
    out.write(__doc__)

def parse_config_file(config_file):
    """Return a dictionary mapping fields in CONFIG_FILE to their values."""
    # Return a newly-created dictionary, rather than the ConfigParser
    # object itself, because we want to avoid callers having to pass
    # the 'default' section name every time they access a field.
    config = {}
    config_parser = configparser.ConfigParser()
    parsed_files = config_parser.read(config_file)
    # ConfigParser.read() returns the number of files read, and if
    # some of them aren't present, it doesn't raise any exceptions
    # about that, it just moves on to try the next one in the list. 
    # (Yes, one could pass it a list of files instead of a single
    # filename.)  Because of this unusual behavior, explained more in
    # https://docs.python.org/2/library/configparser.html, we can't
    # count on an exception being raised and therefore must manually
    # check the returned list instead.
    if len(parsed_files) == 0:
        raise IOError("failed to read config file '%s'" % config_file)
    elif parsed_files[0] != config_file:
        raise IOError("parsed unexpected config file instead of '%s'" % config_file)
    # We have successfully read the config file, so parse it.
    for option in config_parser.options('default'):
        config[option] = config_parser.get('default', option)
    return config


def main():
    """
    By default, creates wiki pages from a supplied CSV.  Optionally,
    deletes those pages instead.

    """
    try:
        opts, args = getopt.getopt(sys.argv[1:], 
                                   'h?d:c:',
                                   ["help",
                                    "usage",
                                    "null-as-value",
                                    "pare=",
                                    "delete-matching=",
                                    "config=",
                                    "show-columns"])
    except getopt.GetoptError as err:
        sys.stderr.write("ERROR: '%s'\n" % err)
        usage(errout=True)
        sys.exit(2)

    csv_in = None
    config = None
    wiki_sess = None
    delete_matching = None
    bad_opt_seen = False
    null_as_value = False
    pare = None
    show_columns = False
    for o, a in opts:
        if o in ("-h", "-?", "--help", "--usage",):
            usage()
            sys.exit(0)
        elif o in ("-d", "--delete-matching",):
            delete_matching = a
        elif o in ("--null-as-value",):
            null_as_value = True
        elif o in ("--pare",):
            pare = int(a)
        elif o in ("--show-columns",):
            show_columns = True
        elif o in ("-c", "--config",):
            config = parse_config_file(a)
        else:
            sys.stderr.write("ERROR: unrecognized option '%s'\n" % o)
            bad_opt_seen = True

    if len(args) < 1:
        sys.stderr.write("ERROR: missing CSV_FILE argument\n")
        sys.exit(2)
    elif len(args) > 1:
        sys.stderr.write("ERROR: too many arguments; expected only CSV_FILE\n")
        sys.exit(2)

    if config is None and not show_columns:
        sys.stderr.write("ERROR: missing config file; use -c to supply it\n")
        usage(errout=True)
        sys.exit(2)

    if show_columns and (config or pare or delete_matching):
        sys.stderr.write("ERROR: --show-columns precludes any other options\n")
        usage(errout=True)
        sys.exit(2)

    if bad_opt_seen:
        sys.exit(2)

    csv_in = CSVInput(args[0], config)

    if show_columns:
        csv_in.show_columns(sys.stdout)
        sys.exit(0)

    wiki_sess = WikiSession(config, csv_in)

    if delete_matching is not None:
        if len(args) > 0:
            sys.stderr.write("ERROR: too many arguments for --delete-matching\n")
            usage(errout=True)
            sys.exit(2)
        try:
            delete_pages(wiki_sess, delete_matching)
        except IndexError as err:
            sys.stderr.write("ERROR: '%s'\n" % err)
            usage(errout=True)
            sys.exit(1)
    else:
        if len(args) < 1:
            sys.stderr.write("ERROR: missing CSV file argument\n")
            usage(errout=True)
            sys.exit(2)
        elif len(args) > 1:
            sys.stderr.write("ERROR: too many arguments\n")
            usage(errout=True)
            sys.exit(2)
        try:
            create_pages(wiki_sess, csv_in, null_as_value, pare)
        except IndexError as err:
            sys.stderr.write("ERROR: '%s'\n" % err)
            usage(errout=True)
            sys.exit(1)


if __name__ == '__main__':
    main()
